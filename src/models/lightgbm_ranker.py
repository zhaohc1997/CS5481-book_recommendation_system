# src/models/lightgbm_ranker.py

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
import pickle
from tqdm import tqdm
from collections import defaultdict


class LightGBMRanker:
    """LightGBM æ’åºæ¨¡å‹ - æ”¯æŒ Embedding ç‰¹å¾ä¸å…¨é‡æ‰“åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self, n_estimators=100, learning_rate=0.05):
        self.model = None
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.books_df = None
        self.ratings_df = None
        
        # ç‰¹å¾ç¼“å­˜
        self.book_features = {}
        self.isbn_stats = {}
        self.cooccurrence = {}
        
        # ä½œè€…å’Œå‡ºç‰ˆå•†ç¼–ç 
        self.author_to_id = {}
        self.publisher_to_id = {}
        
        # Embedding å‘é‡ç¼“å­˜
        self.embeddings = {} 
    
    def set_embeddings(self, embedding_dict):
        """
        æ³¨å…¥é¢„è®­ç»ƒçš„ Embedding
        å‚æ•°: embedding_dict: {isbn: vector} å­—å…¸
        """
        self.embeddings = embedding_dict
        print(f"  âœ“ LightGBM å·²åŠ è½½ {len(self.embeddings):,} ä¸ª Embedding å‘é‡")
        
        if self.embeddings:
            # æ£€æŸ¥å‘é‡ç»´åº¦
            sample_isbn = list(self.embeddings.keys())[0]
            sample_vector = self.embeddings[sample_isbn]
            vector_dim = len(sample_vector)
            print(f"  âœ“ Embedding ç»´åº¦: {vector_dim}")
    
    def _calculate_embedding_similarity(self, isbn_a, isbn_b):
        """
        è®¡ç®—ä¸¤æœ¬ä¹¦çš„å‘é‡ç›¸ä¼¼åº¦ (Cosine Similarity)
        è¿”å›: float ç›¸ä¼¼åº¦åˆ†æ•° [-1, 1]
        """
        if not self.embeddings:
            return 0.0
        
        vec_a = self.embeddings.get(isbn_a)
        vec_b = self.embeddings.get(isbn_b)
        
        if vec_a is None or vec_b is None:
            return 0.0
        
        try:
            # Cosine Similarity: (A . B) / (|A| * |B|)
            dot_product = np.dot(vec_a, vec_b)
            norm_a = np.linalg.norm(vec_a)
            norm_b = np.linalg.norm(vec_b)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
            
            return dot_product / (norm_a * norm_b)
        except:
            return 0.0

    def _compute_book_features(self, books_df, ratings_df):
        """ä¸ºæ¯æœ¬ä¹¦è®¡ç®—ç»Ÿè®¡ç‰¹å¾"""
        print("  è®¡ç®—å›¾ä¹¦ç‰¹å¾...")
        
        # è¯„åˆ†ç»Ÿè®¡
        rating_stats = ratings_df[ratings_df['Book-Rating'] > 0].groupby('ISBN').agg({
            'Book-Rating': ['mean', 'count', 'std', 'min', 'max'],
            'User-ID': 'nunique'
        }).reset_index()
        
        rating_stats.columns = ['ISBN', 'avg_rating', 'rating_count', 'rating_std', 
                                'min_rating', 'max_rating', 'unique_users']
        rating_stats['rating_std'] = rating_stats['rating_std'].fillna(0)
        
        # åˆå¹¶åˆ°å›¾ä¹¦æ•°æ®
        books_with_stats = books_df.merge(rating_stats, on='ISBN', how='left')
        
        # å¡«å……ç¼ºå¤±å€¼
        global_avg_rating = ratings_df[ratings_df['Book-Rating'] > 0]['Book-Rating'].mean()
        books_with_stats['avg_rating'] = books_with_stats['avg_rating'].fillna(global_avg_rating)
        books_with_stats['rating_count'] = books_with_stats['rating_count'].fillna(0)
        books_with_stats['rating_std'] = books_with_stats['rating_std'].fillna(0)
        books_with_stats['unique_users'] = books_with_stats['unique_users'].fillna(0)
        books_with_stats['min_rating'] = books_with_stats['min_rating'].fillna(0)
        books_with_stats['max_rating'] = books_with_stats['max_rating'].fillna(0)
        
        # æ„å»ºä½œè€…å’Œå‡ºç‰ˆå•†ç¼–ç 
        unique_authors = books_with_stats['Book-Author'].unique()
        unique_publishers = books_with_stats['Publisher'].unique()
        
        self.author_to_id = {author: idx for idx, author in enumerate(unique_authors)}
        self.publisher_to_id = {pub: idx for idx, pub in enumerate(unique_publishers)}
        
        # å­˜å‚¨ç‰¹å¾
        for _, row in books_with_stats.iterrows():
            isbn = row['ISBN']
            
            author = row.get('Book-Author', 'Unknown')
            publisher = row.get('Publisher', 'Unknown')
            year = row.get('Year-Of-Publication', 2000)
            
            # å¤„ç†å¼‚å¸¸å¹´ä»½
            try:
                year = int(year)
                if year < 1900 or year > 2025:
                    year = 2000
            except:
                year = 2000
            
            # book_features
            self.book_features[isbn] = {
                'avg_rating': row['avg_rating'],
                'rating_count': row['rating_count'],
                'rating_std': row['rating_std'],
                'min_rating': row['min_rating'],
                'max_rating': row['max_rating'],
                'unique_users': row['unique_users'],
                'popularity': np.log1p(row['rating_count']),
                'author': author,
                'publisher': publisher,
                'year': year,
                'author_id': self.author_to_id.get(author, 0),
                'publisher_id': self.publisher_to_id.get(publisher, 0),
            }
            
            # isbn_stats (è¾…åŠ©)
            self.isbn_stats[isbn] = {
                'rating_count': row['rating_count'],
                'avg_rating': row['avg_rating'],
                'rating_std': row['rating_std'],
                'user_count': row['unique_users']
            }
        
        print(f"  âœ“ è®¡ç®—äº† {len(self.book_features)} æœ¬ä¹¦çš„ç‰¹å¾")
        print(f"  âœ“ å”¯ä¸€ä½œè€…æ•°: {len(self.author_to_id)}")
        print(f"  âœ“ å”¯ä¸€å‡ºç‰ˆå•†æ•°: {len(self.publisher_to_id)}")
    
    def _compute_cooccurrence(self, ratings_df):
        """è®¡ç®—å›¾ä¹¦å…±ç°çŸ©é˜µ"""
        print("  è®¡ç®—å›¾ä¹¦å…±ç°ç‰¹å¾...")
        
        user_books = ratings_df[ratings_df['Book-Rating'] >= 7].groupby('User-ID')['ISBN'].apply(list)
        
        cooccurrence = {}
        for user_id, books in tqdm(user_books.items(), desc="  è®¡ç®—å…±ç°"):
            if len(books) < 2:
                continue
            
            # ä»…åœ¨ä¸€å®šçª—å£å†…æˆ–å…¨é‡è®¡ç®—
            for i in range(len(books)):
                for j in range(i + 1, len(books)):
                    book_a, book_b = books[i], books[j]
                    
                    # æ’åº key ä¿è¯ä¸€è‡´æ€§
                    key = tuple(sorted((book_a, book_b)))
                    cooccurrence[key] = cooccurrence.get(key, 0) + 1
        
        self.cooccurrence = cooccurrence
        print(f"  âœ“ è®¡ç®—äº† {len(cooccurrence):,} å¯¹å›¾ä¹¦å…±ç°")
    
    def _extract_pairwise_features(self, isbn_a, isbn_b):
        """
        æå–æˆå¯¹ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆ - 16ä¸ªç‰¹å¾ï¼‰
        åŒ…å«ï¼šå±æ€§åŒ¹é…ã€ç›®æ ‡ä¹¦ç»Ÿè®¡ã€å¯¹æ¯”ç‰¹å¾ã€å…±ç°ç‰¹å¾ã€äº¤å‰ç‰¹å¾ã€Embeddingç›¸ä¼¼åº¦
        """
        features = []
        
        feat_a = self.book_features.get(isbn_a, {})
        feat_b = self.book_features.get(isbn_b, {})
        
        # === 1-3. å±æ€§åŒ¹é…ç‰¹å¾ ===
        same_author = 1 if feat_a.get('author') == feat_b.get('author') else 0
        features.append(same_author)
        
        same_publisher = 1 if feat_a.get('publisher') == feat_b.get('publisher') else 0
        features.append(same_publisher)
        
        year_diff = abs(feat_a.get('year', 2000) - feat_b.get('year', 2000))
        features.append(min(year_diff / 10.0, 10.0))  # å½’ä¸€åŒ–åˆ° [0, 10]
        
        # === 4-7. å€™é€‰ä¹¦ç»Ÿè®¡ç‰¹å¾ ===
        features.append(feat_b.get('avg_rating', 0))                    # 4. å¹³å‡è¯„åˆ†
        features.append(feat_b.get('popularity', 0))                    # 5. æµè¡Œåº¦ï¼ˆå¯¹æ•°ï¼‰
        features.append(feat_b.get('rating_std', 0))                    # 6. è¯„åˆ†æ ‡å‡†å·®
        
        rating_range = feat_b.get('max_rating', 0) - feat_b.get('min_rating', 0)
        features.append(rating_range)                                   # 7. è¯„åˆ†èŒƒå›´
        
        # === 8-10. æˆå¯¹å¯¹æ¯”ç‰¹å¾ ===
        rating_diff = abs(feat_a.get('avg_rating', 0) - feat_b.get('avg_rating', 0))
        features.append(rating_diff)                                    # 8. è¯„åˆ†å·®å¼‚
        
        pop_a = feat_a.get('popularity', 0) + 1
        pop_b = feat_b.get('popularity', 0) + 1
        features.append(pop_b / pop_a)                                  # 9. æµè¡Œåº¦æ¯”ä¾‹
        
        rating_sim = 1.0 - min(rating_diff / 10.0, 1.0)                 # 10. è¯„åˆ†ç›¸ä¼¼åº¦
        features.append(rating_sim)
        
        # === 11-13. å…±ç°ç‰¹å¾ ===
        # æ³¨æ„ï¼škey éœ€è¦æ’åº
        co_key = tuple(sorted((isbn_a, isbn_b)))
        cooccur_count = self.cooccurrence.get(co_key, 0)
        
        features.append(cooccur_count)                                  # 11. å…±ç°æ¬¡æ•°
        features.append(np.log1p(cooccur_count))                        # 12. å…±ç°å¯¹æ•°
        
        if feat_a.get('unique_users', 0) > 0:
            cooccur_ratio = cooccur_count / feat_a.get('unique_users', 1)
        else:
            cooccur_ratio = 0
        features.append(cooccur_ratio)                                  # 13. å…±ç°æ¯”ä¾‹
        
        # === 14-15. äº¤å‰ç‰¹å¾ ===
        pop_product = np.log1p(
            feat_a.get('rating_count', 0) * feat_b.get('rating_count', 0)
        )
        features.append(pop_product)                                    # 14. æµè¡Œåº¦ä¹˜ç§¯
        
        hotness_a = feat_a.get('avg_rating', 0) * np.log1p(feat_a.get('rating_count', 0))
        hotness_b = feat_b.get('avg_rating', 0) * np.log1p(feat_b.get('rating_count', 0))
        features.append(hotness_b / max(hotness_a, 1))                  # 15. çƒ­é—¨åº¦æ¯”ä¾‹
        
        # === 16. Embedding ç›¸ä¼¼åº¦ ===
        emb_sim = self._calculate_embedding_similarity(isbn_a, isbn_b)
        features.append(emb_sim)                                        # 16. å‘é‡ç›¸ä¼¼åº¦
        
        return features
    
    def train(self, ratings_df, books_df, users_df=None):
        """è®­ç»ƒ LightGBM æ’åºæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("="*60)
        print("è®­ç»ƒ LightGBM æ’åºæ¨¡å‹ï¼ˆç²¾æ’é˜¶æ®µ - Embedding å¢å¼ºç‰ˆï¼‰")
        print("="*60)
        
        self.books_df = books_df.copy()
        self.ratings_df = ratings_df.copy()
        
        # è®¡ç®—å›¾ä¹¦ç‰¹å¾
        self._compute_book_features(books_df, ratings_df)
        
        # è®¡ç®—å…±ç°çŸ©é˜µ
        self._compute_cooccurrence(ratings_df)
        
        # === æ„é€ è®­ç»ƒæ ·æœ¬ ===
        print("\næ„é€ è®­ç»ƒæ ·æœ¬...")
        X_train = []
        y_train = []
        
        user_book_map = ratings_df[ratings_df['Book-Rating'] >= 7].groupby('User-ID')['ISBN'].apply(list)
        
        sample_count = 0
        max_samples = 50000
        
        for user_id, liked_books in tqdm(user_book_map.items(), desc="  ç”Ÿæˆæ ·æœ¬"):
            if sample_count >= max_samples:
                break
            
            if len(liked_books) < 2:
                continue
            
            liked_books_set = set(liked_books)
            
            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ ·æœ¬
            for i in range(min(len(liked_books), 15)):
                if sample_count >= max_samples:
                    break
                
                book_a = liked_books[i]
                
                # æ­£æ ·æœ¬ï¼šç”¨æˆ·ä¹Ÿå–œæ¬¢çš„å…¶ä»–ä¹¦
                for j in range(min(len(liked_books), 15)):
                    if i == j:
                        continue
                    
                    book_b = liked_books[j]
                    features = self._extract_pairwise_features(book_a, book_b)
                    
                    if features and len(features) == 16:  # âœ… ç¡®ä¿16ä¸ªç‰¹å¾
                        X_train.append(features)
                        y_train.append(1)
                        sample_count += 1
                    
                    if sample_count >= max_samples:
                        break
                
                # è´Ÿæ ·æœ¬ï¼šéšæœºé€‰æ‹©ç”¨æˆ·æ²¡è¯„åˆ†çš„ä¹¦
                neg_samples = 3
                all_books = list(self.book_features.keys())
                
                # ç®€å•éšæœºé‡‡æ ·
                for _ in range(neg_samples):
                    book_b = np.random.choice(all_books)
                    if book_b in liked_books_set:
                        continue
                        
                    features = self._extract_pairwise_features(book_a, book_b)
                    
                    if features and len(features) == 16:
                        X_train.append(features)
                        y_train.append(0)
                        sample_count += 1
        
        X_train = np.array(X_train)
        y_train = np.array(y_train)
        
        print(f"\nâœ“ è®­ç»ƒæ ·æœ¬: {len(X_train):,}")
        print(f"âœ“ ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
        print(f"âœ“ æ­£æ ·æœ¬æ¯”ä¾‹: {y_train.mean():.2%}")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_tr, X_val, y_tr, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # è®­ç»ƒ LightGBM
        train_data = lgb.Dataset(X_tr, label=y_tr)
        val_data = lgb.Dataset(X_val, label=y_val)
        
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': self.learning_rate,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'min_data_in_leaf': 20,
        }
        
        print("\nè®­ç»ƒ LightGBM...")
        self.model = lgb.train(
            params,
            train_data,
            num_boost_round=self.n_estimators,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(20),
                lgb.log_evaluation(20)
            ]
        )
        
        # ç‰¹å¾é‡è¦æ€§
        feature_names = [
            'same_author', 'same_publisher', 'year_diff',
            'b_avg_rating', 'b_popularity', 'b_rating_std', 'b_rating_range',
            'rating_diff', 'popularity_ratio', 'rating_similarity',
            'cooccur_count', 'cooccur_log', 'cooccur_ratio',
            'popularity_product', 'hotness_ratio',
            'embedding_similarity'  # âœ… ç‰¹å¾16
        ]
        
        print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§ Top 10:")
        importance = self.model.feature_importance()
        importances = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)
        for name, imp in importances[:10]:
            print(f"  {name:25s}: {imp:8.1f}")
        
        print("\nâœ… LightGBM æ’åºæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå¢å¼ºç‰ˆï¼‰")
    
    def recommend(self, book_title, n=10, candidate_pool=None):
        """
        æ¨èå›¾ä¹¦ï¼ˆå¢å¼ºç‰ˆï¼šè¿”å›å®Œæ•´é¢„æµ‹åˆ—è¡¨ï¼Œä¸æˆªæ–­ï¼‰
        """
        if self.model is None:
            return []
        
        if self.books_df is None:
            return []
        
        # è·å–æŸ¥è¯¢å›¾ä¹¦çš„ ISBN
        query_book = self.books_df[self.books_df['Book-Title'] == book_title]
        if query_book.empty:
            return []
        
        query_isbn = query_book.iloc[0]['ISBN']
        
        # ç¡®å®šå€™é€‰é›†
        if candidate_pool:
            candidates = [isbn for isbn in candidate_pool if isbn != query_isbn]
        else:
            # å¦‚æœæ²¡æœ‰æä¾›å€™é€‰æ± ï¼Œç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸ºæ˜¯ Rankerï¼‰ï¼Œä½†å¯ä»¥å…œåº•
            candidates = []
        
        if not candidates:
            return []
        
        # === æ„å»ºç‰¹å¾ ===
        X = []
        valid_candidates = []
        
        for candidate_isbn in candidates:
            features = self._extract_pairwise_features(query_isbn, candidate_isbn)
            
            if features is not None and len(features) == 16:
                X.append(features)
                valid_candidates.append(candidate_isbn)
        
        if not X:
            return []
        
        X = np.array(X)
        
        # === é¢„æµ‹ ===
        try:
            scores = self.model.predict(X)
        except Exception as e:
            print(f"  âŒ é¢„æµ‹å¤±è´¥: {e}")
            return []
        
        # === è¿”å›å®Œæ•´ç»“æœ (äº¤ç»™ TwoStage è¿›è¡Œæˆªæ–­å’Œæ··åˆ) ===
        recommendations = []
        
        # æˆ‘ä»¬ä¸åœ¨è¿™é‡Œåšæˆªæ–­ï¼Œè€Œæ˜¯è¿”å›æ‰€æœ‰æœ‰æ•ˆå€™é€‰çš„é¢„æµ‹åˆ†
        # æ’åºå¯ä»¥åœ¨è¿™é‡Œåšï¼Œä¹Ÿå¯ä»¥åœ¨ TwoStage åšï¼Œä½†è¿™é‡Œåšä¸€ä¸‹æ¯”è¾ƒæ–¹ä¾¿
        sorted_indices = np.argsort(scores)[::-1]
        
        # å³ä½¿è¯·æ±‚äº† nï¼Œå¦‚æœæä¾›äº† candidate_poolï¼Œæˆ‘ä»¬æœ€å¥½ä¹Ÿè¿”å›æ›´å¤šç»“æœ
        # ä½†ä¸ºäº† API å…¼å®¹æ€§ï¼Œæˆ‘ä»¬è‡³å°‘è¿”å› n ä¸ªï¼Œæˆ–è€…å…¨éƒ¨
        # é‰´äº TwoStage çš„ä¼˜åŒ–é€»è¾‘ï¼Œæˆ‘ä»¬è¿”å› *æ‰€æœ‰* è®¡ç®—äº†åˆ†æ•°çš„å€™é€‰
        
        for idx in sorted_indices:
            candidate_isbn = valid_candidates[idx]
            book_info = self.books_df[self.books_df['ISBN'] == candidate_isbn]
            
            if not book_info.empty:
                recommendations.append({
                    'title': book_info.iloc[0]['Book-Title'],
                    'author': book_info.iloc[0].get('Book-Author', 'Unknown'),
                    'score': float(scores[idx])
                })
        
        return recommendations

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        model_data = {
            'model': self.model,
            'cooccurrence': getattr(self, 'cooccurrence', {}),
            'isbn_stats': getattr(self, 'isbn_stats', {}),
            'book_features': getattr(self, 'book_features', {}),
            'author_to_id': getattr(self, 'author_to_id', {}),
            'publisher_to_id': getattr(self, 'publisher_to_id', {}),
            'books_df': self.books_df,
            'n_estimators': self.n_estimators,
            'learning_rate': self.learning_rate,
            'embeddings': self.embeddings 
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"LightGBM æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        if isinstance(model_data, dict):
            self.model = model_data.get('model')
            self.cooccurrence = model_data.get('cooccurrence', {})
            self.isbn_stats = model_data.get('isbn_stats', {})
            self.book_features = model_data.get('book_features', {})
            self.author_to_id = model_data.get('author_to_id', {})
            self.publisher_to_id = model_data.get('publisher_to_id', {})
            self.books_df = model_data.get('books_df')
            self.n_estimators = model_data.get('n_estimators', 100)
            self.learning_rate = model_data.get('learning_rate', 0.05)
            self.embeddings = model_data.get('embeddings', {})
            
            print(f"LightGBM æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            print(f"  âœ“ Embedding: {len(self.embeddings):,}")
        else:
            self.model = model_data
            print("âš ï¸ åŠ è½½äº†æ—§æ ¼å¼æ¨¡å‹")